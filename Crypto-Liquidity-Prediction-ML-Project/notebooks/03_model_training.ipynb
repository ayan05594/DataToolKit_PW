{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6cff423",
   "metadata": {},
   "source": [
    "# Loading Feature-Engineered Data for Model Training\n",
    "\n",
    "Before starting model training, we first load the feature-engineered dataset prepared in the previous step.  \n",
    "This dataset contains all the processed features necessary for training the predictive models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6d2d26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             coin symbol         price     1h    24h     7d    24h_volume  \\\n",
      "0         Bitcoin    BTC  40859.460000  0.022  0.030  0.055  3.539076e+10   \n",
      "1   Origin Dollar   OUSD      0.993428  0.001 -0.002  0.001  8.863360e+05   \n",
      "2  Iron Bank EURO  IBEUR      1.080000  0.000 -0.004  0.009  9.525810e+04   \n",
      "3       Prometeus   PROM      7.960000  0.017  0.008  0.015  1.069360e+06   \n",
      "4    MaidSafeCoin   MAID      0.294920  0.023  0.010  0.045  3.041720e+03   \n",
      "\n",
      "        mkt_cap        date   price_MA_2d  market_cap_MA_2d  volatility_score  \\\n",
      "0  7.709915e+11  2022-03-16           NaN               NaN             0.008   \n",
      "1  1.503384e+08  2022-03-16  20430.226714      3.855709e+11             0.003   \n",
      "2  1.300442e+08  2022-03-16      1.036714      1.401913e+08             0.004   \n",
      "3  1.302007e+08  2022-03-16      4.520000      1.301224e+08             0.009   \n",
      "4  1.327759e+08  2022-03-16      4.127460      1.314883e+08             0.013   \n",
      "\n",
      "   liquidity_ratio  \n",
      "0         0.045903  \n",
      "1         0.005896  \n",
      "2         0.000733  \n",
      "3         0.008213  \n",
      "4         0.000023  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the feature-engineered dataset\n",
    "file_path = r\"C:\\Users\\ACER\\OneDrive\\Documents\\my codess\\Data-Analytics-Assignment\\Crypto-Liquidity-Prediction-ML-Project\\data\\processed\\crypto_data_feature_engineered.csv\"\n",
    "\n",
    "# Loading the dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Displaying the first few rows to verify successful loading\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2b1996",
   "metadata": {},
   "source": [
    "# 03. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a86360",
   "metadata": {},
   "source": [
    "### 3.1 Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc7de14",
   "metadata": {},
   "source": [
    "We are predicting a continuous variable (liquidity ratio), so we use **Regression models**.\n",
    "Start with **Linear Regression** as a simple baseline before trying complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbab973",
   "metadata": {},
   "source": [
    "### 3.2 Preparing Data for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e331761",
   "metadata": {},
   "source": [
    "In this step, we prepare the dataset for building and evaluating a machine learning model. The features (independent variables) and the target (dependent variable) are separated. We then split the dataset into training and testing subsets — the training data is used to train the model, and the testing data is used to evaluate how well the model performs on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5edbff49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (793, 4)\n",
      "Shape of testing data: (199, 4)\n"
     ]
    }
   ],
   "source": [
    "# Corrected code for Ethereum dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Selecting correct features based on your dataset\n",
    "X = df[['price', '24h_volume', 'mkt_cap', 'volatility_score']]\n",
    "y = df['liquidity_ratio']\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Shape of training data:\", X_train.shape)\n",
    "print(\"Shape of testing data:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c17c9b",
   "metadata": {},
   "source": [
    "### 3.3 Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6699d18",
   "metadata": {},
   "source": [
    "In this step, we train a machine learning model using the training dataset `(X_train, y_train)` that we prepared earlier. The model will learn the underlying patterns and relationships between the input features (like price, volume, market cap, volatility) and the target variable `(liquidity_ratio)`.\n",
    "For this example, we'll use a Linear Regression model — a basic and widely used algorithm for regression tasks that assumes a linear relationship between inputs and output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eb9ffc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Coefficients: [-8.43524745e-07  1.67091127e-11 -6.43485972e-13  2.17238199e+00]\n",
      "Model Intercept: 0.03300684393692446\n",
      "----------------------------------------------------\n",
      "Model is trained successfully.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Linear Regression model from scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initializing the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Training (fit) the model using the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Printing model coefficients which is usually not requied\n",
    "# but can be useful for understanding the model\n",
    "print(\"Model Coefficients:\", model.coef_)\n",
    "print(\"Model Intercept:\", model.intercept_)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"Model is trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a0aee",
   "metadata": {},
   "source": [
    "### 3.4 Model Evaluation (Initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbbe832",
   "metadata": {},
   "source": [
    "In this step, we evaluate the initial performance of our trained model using the test dataset. This evaluation provides insight into how well the model is able to predict unseen data. \n",
    "\n",
    "We calculate key regression metrics such as:\n",
    "\n",
    "- **Root Mean Squared Error (RMSE):** Measures the average size of the errors and penalizes larger errors more heavily.\n",
    "- **Mean Absolute Error (MAE):** Provides the average magnitude of prediction errors.\n",
    "- **R² Score:** Indicates the proportion of variance in the target variable explained by the model.\n",
    "\n",
    "A higher R² score and lower error values indicate better model performance. The results from this initial evaluation help us understand if the model is making accurate predictions or if further improvements — like hyperparameter tuning or feature selection — are necessary to enhance its predictive ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8760a6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.44150904133427954\n",
      "MAE: 0.12048422855312403\n",
      "R-squared: 0.049835303442470336\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Generate predictions from the test dataset\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "rmse_val = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "mae_val = mean_absolute_error(y_test, predictions)\n",
    "r2_val = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"RMSE: {rmse_val}\")\n",
    "print(f\"MAE: {mae_val}\")\n",
    "print(f\"R-squared: {r2_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273c8a5d",
   "metadata": {},
   "source": [
    "**From the output we see:**\n",
    "\n",
    "- The **RMSE** value of approximately **0.44** indicates a moderate average prediction error magnitude, meaning predictions deviate from actual values by this amount on average.  \n",
    "- The **MAE** of around **0.12** confirms that the average absolute error is fairly low but still noticeable.  \n",
    "- The **R-squared** value of about **0.05** suggests the model explains only 5% of the variance in the target variable, indicating weak predictive power.  \n",
    "- Overall, the model is **not performing well** and struggles to accurately capture the relationship between features and the target.  \n",
    "- **Further improvements** like feature engineering, hyperparameter tuning, or trying different algorithms are needed to boost performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07139afa",
   "metadata": {},
   "source": [
    "### 3.5 Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6bae35",
   "metadata": {},
   "source": [
    "In this step, we will aim to improve the performance of our machine learning model by tuning its hyperparameters. Hyperparameters are settings that control the learning process, such as the number of trees in a Random Forest or the maximum depth of each tree. Instead of manually trying different combinations, we use **GridSearchCV**, a powerful tool that performs an exhaustive search over a specified parameter grid with cross-validation. This helps us find the best combination of hyperparameters that yields the highest model performance based on a chosen metric (here, R² score). By optimizing these parameters, we expect the model to generalize better on unseen data and improve prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43a794c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "Best Parameters Found: {'max_depth': 10, 'n_estimators': 150}\n",
      "Best cross-validation R² score: 0.7591\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initializing the model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 10, 15]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    scoring='r2',\n",
    "    verbose=1,\n",
    "    refit=True\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Printing the best parameters and the best score\n",
    "print(\"Best Parameters Found:\", grid_search.best_params_)\n",
    "print(f\"Best cross-validation R² score: {grid_search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0701e7c3",
   "metadata": {},
   "source": [
    "**From the output we see:**\n",
    "\n",
    "- GridSearchCV evaluated 27 different hyperparameter combinations using 3-fold cross-validation.\n",
    "- The best parameters found are:\n",
    "  - `max_depth`: 10\n",
    "  - `n_estimators`: 150\n",
    "- The best cross-validation R² score achieved with these parameters is approximately 0.7591.\n",
    "- This indicates that the tuned Random Forest model explains about 75.9% of the variance in the target variable on validation data.\n",
    "- Using these parameters for retraining should significantly improve the model's prediction accuracy and generalization ability compared to default settings.\n",
    "- Further tuning or experimenting with other hyperparameters could potentially enhance the performance even more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46168bbc",
   "metadata": {},
   "source": [
    "### 3.6 Retrain Model with Best Parameters and Re-evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e14d9",
   "metadata": {},
   "source": [
    "Once the best hyperparameters are found using GridSearchCV, the next thing is to retrain the model with these optimal parameters on the entire training dataset. This enables the model to learn using the optimized settings that have performed the best across cross-validation. After retraining, the model is again tested using the test dataset in order to determine its prediction accuracy and ability to generalize to new data. We employ the same performance metrics—Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R² Score—to measure the improvement realized through hyperparameter tuning. This verifies whether the tuning has improved the model's capability to produce good and reliable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "079f8437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Root Mean Squared Error (RMSE): 0.3467\n",
      "Updated Mean Absolute Error (MAE): 0.0504\n",
      "Updated R-squared (R²) Score: 0.4140\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Retraining the model with the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "best_model = RandomForestRegressor(\n",
    "    n_estimators=best_params['n_estimators'], \n",
    "    max_depth=best_params['max_depth'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fitting the model on the training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Evaluating model performance\n",
    "rmse_best = np.sqrt(mean_squared_error(y_test, y_pred_best))\n",
    "mae_best = mean_absolute_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Updated Root Mean Squared Error (RMSE): {rmse_best:.4f}\")\n",
    "print(f\"Updated Mean Absolute Error (MAE): {mae_best:.4f}\")\n",
    "print(f\"Updated R-squared (R²) Score: {r2_best:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ebb0db",
   "metadata": {},
   "source": [
    "**Model Evaluation After Hyperparameter Tuning**\n",
    "\n",
    "- The updated RMSE of approximately **0.35** indicates a reduction in the average magnitude of prediction errors compared to before, showing improved accuracy.\n",
    "- The MAE value of **0.05** reflects that the average absolute error has significantly decreased, meaning predictions are closer to actual values.\n",
    "- The R-squared score of **0.41** suggests the model now explains about 41% of the variance in the target variable, which is a notable improvement in predictive power.\n",
    "- Overall, the hyperparameter tuning has enhanced the model’s performance, but there is still room for improvement to achieve higher accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea6de34",
   "metadata": {},
   "source": [
    "### 3.7 Final Prediction Check (Compare Actual vs Predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12251b7",
   "metadata": {},
   "source": [
    "In this step, we will compare the actual target values with the predicted values from our tuned model on the test dataset. This comparison will helps us visually inspect how close the model’s predictions are to the real data points. By analyzing these results, we can identify if the model is consistently accurate across the range of values or if there are specific areas where it struggles. Such insights can guide further model improvements or validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0f36bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Actual Liquidity  Predicted Liquidity     Error\n",
      "0          0.051516             0.053734 -0.002219\n",
      "1          0.080784             0.068837  0.011947\n",
      "2          0.064324             0.067555 -0.003231\n",
      "3          0.153632             0.156490 -0.002859\n",
      "4          0.010830             0.011185 -0.000356\n",
      "5          0.123382             0.131801 -0.008419\n",
      "6          0.219153             0.189631  0.029522\n",
      "7          0.007353             0.007230  0.000122\n",
      "8          0.003577             0.004392 -0.000816\n",
      "9          0.141715             0.142974 -0.001258\n"
     ]
    }
   ],
   "source": [
    "# Predicting on test data\n",
    "predicted_liquidity = best_model.predict(X_test)\n",
    "\n",
    "# Created comparison DataFrame with reset index\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual Liquidity': y_test.values,\n",
    "    'Predicted Liquidity': predicted_liquidity\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Add error column\n",
    "comparison_df['Error'] = comparison_df['Actual Liquidity'] - comparison_df['Predicted Liquidity']\n",
    "\n",
    "print(comparison_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef1eb2",
   "metadata": {},
   "source": [
    "**Conclusion: Actual vs Predicted Liquidity Comparison**\n",
    "\n",
    "The comparison between actual and predicted liquidity values shows that the model's predictions are generally close to the true values, with most errors being small in magnitude. The calculated errors indicate the difference between the predicted and actual liquidity for each data point, revealing the model's precision on individual samples. While some points exhibit slightly higher errors, overall the prediction deviations are minor, demonstrating that the model captures the underlying patterns in the data reasonably well. This output confirms the improved accuracy achieved after hyperparameter tuning, though continuous evaluation and further refinement could help reduce these errors even more for robust real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc6702b",
   "metadata": {},
   "source": [
    "### 3.8 Saving the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f566cd48",
   "metadata": {},
   "source": [
    "Once the model is trained and tuned to achieve satisfactory performance, the next important step is to save the model to disk. Saving the model allows you to reuse it later without retraining, which is essential for deployment or further analysis. The saved model can be loaded at any time to make predictions on new data.\n",
    "\n",
    "Common libraries used for saving models in Python are  `joblib` and `pickle`. `joblib` is often preferred for saving large models efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully at: \n",
      "C:\\Users\\ACER\\OneDrive\\Documents\\my codess\\Data-Analytics-Assignment\\Crypto-Liquidity-Prediction-ML-Project\\models\\crypto_liquidity_rf_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "model = best_model\n",
    "\n",
    "# path to save the model\n",
    "model_save_path = r\"C:\\Users\\ACER\\OneDrive\\Documents\\my codess\\Data-Analytics-Assignment\\Crypto-Liquidity-Prediction-ML-Project\\models\\crypto_liquidity_rf_model.pkl\"\n",
    "\n",
    "# This will Create a directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, model_save_path)\n",
    "\n",
    "print(f\"Model saved successfully at: \\n{model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cf9ec5",
   "metadata": {},
   "source": [
    "### 3.9 Loading and Testing the Saved Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e608cdd5",
   "metadata": {},
   "source": [
    "After saving the trained machine learning model, it is important to verify that the model can be successfully loaded and used to make predictions. This step ensures that the saved model file is intact and the model's predictive capabilities remain consistent. Loading the model from disk allows you to reuse the model in different environments without retraining, facilitating deployment and future inference tasks.\n",
    "\n",
    "In this step, we load the saved model file and use it to predict on the test dataset. We then evaluate the predictions by calculating error metrics such as `RMSE` and `R² score` to confirm the model’s performance remains reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112d8a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: C:\\Users\\ACER\\OneDrive\\Documents\\my codess\\Data-Analytics-Assignment\\Crypto-Liquidity-Prediction-ML-Project\\outputs\\models\\crypto_liquidity_rf_model.pkl\n",
      "Loaded Model RMSE: 0.3467\n",
      "Loaded Model R² Score: 0.4140\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define model save path\n",
    "model_dir = r\"C:\\Users\\ACER\\OneDrive\\Documents\\my codess\\Data-Analytics-Assignment\\Crypto-Liquidity-Prediction-ML-Project\\outputs\\models\"\n",
    "model_path = os.path.join(model_dir, \"crypto_liquidity_rf_model.pkl\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "print(f\"Model saved at: {model_path}\")\n",
    "\n",
    "# Load the model from the specified path\n",
    "loaded_model = joblib.load(model_path)\n",
    "\n",
    "# Using the loaded model to predict on test data\n",
    "y_pred_loaded = loaded_model.predict(X_test)\n",
    "\n",
    "# Calculating evaluation metrics to verify performance\n",
    "rmse_loaded = np.sqrt(mean_squared_error(y_test, y_pred_loaded))\n",
    "r2_loaded = r2_score(y_test, y_pred_loaded)\n",
    "\n",
    "# Printing the evaluation results\n",
    "print(f\"Loaded Model RMSE: {rmse_loaded:.4f}\")\n",
    "print(f\"Loaded Model R² Score: {r2_loaded:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32807895",
   "metadata": {},
   "source": [
    "# Summary of `03_Model_Training.ipynb`\n",
    "\n",
    "This notebook covers the complete lifecycle of building a predictive model for Crypto Liquidity:\n",
    "\n",
    "## 1. Data Preparation\n",
    "- Loaded and preprocessed the dataset.\n",
    "- Split the data into training and testing sets.\n",
    "\n",
    "## 2. Initial Model Training\n",
    "- Trained a baseline Random Forest Regressor model using default parameters.\n",
    "- Evaluated initial performance using RMSE, MAE, and R² metrics.\n",
    "\n",
    "## 3. Hyperparameter Tuning\n",
    "- Applied GridSearchCV to identify the best hyperparameters (`n_estimators` and `max_depth`).\n",
    "- Achieved improved model performance with optimized parameters.\n",
    "\n",
    "## 4. Retraining and Evaluation\n",
    "- Retrained the model using the best hyperparameters.\n",
    "- Re-evaluated the model and observed significant improvement in accuracy.\n",
    "\n",
    "## 5. Prediction Comparison\n",
    "- Compared actual vs predicted liquidity values on the test dataset.\n",
    "- Analyzed prediction errors to assess model accuracy and reliability.\n",
    "\n",
    "## 6. Model Saving\n",
    "- Saved the trained model as a pickle file in a dedicated `models` directory.\n",
    "- This facilitates easy loading for future inference or deployment.\n",
    "\n",
    "---\n",
    "\n",
    "# Next Steps: Deployment `app.py`\n",
    "In the deployment, we will:\n",
    "\n",
    "- Load the saved model from the `models` directory.\n",
    "- Prepare the pipeline to accept new input data and generate predictions.\n",
    "- Optionally, create an streamlit interface for practical use of the model.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
