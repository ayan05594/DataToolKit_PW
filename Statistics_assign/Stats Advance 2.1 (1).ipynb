{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "443794d8-731c-4e1a-9c43-fc90efc2555d",
   "metadata": {},
   "source": [
    "Qns 1) Explain the properties of the F-distribution.\n",
    "Ans) Properties of the F-Distribution:\n",
    "The F-distribution is the distribution of the ratio of two independent variances (sample variances) drawn from normal populations. It is commonly used to compare variability across groups.\n",
    "\n",
    "1. Shape: The F-distribution is positively skewed (asymmetric).\n",
    "As the degrees of freedom (numerator and denominator) increase, it becomes more symmetric and approaches the normal distribution.\n",
    "2. Range: The F-distribution only takes positive values, starting from 0 and extending to infinity.\n",
    "3. Degrees of Freedom: It is characterized by two parameters: numerator degrees of freedom (related to the first sample) and denominator degrees of freedom (related to the second sample). These affect its shape.\n",
    "4. Mean and Variance: The mean exists if the denominator degrees of freedom are greater than 2.\n",
    "The variance exists if the denominator degrees of freedom are greater than 4.\n",
    "5. Right-Tailed Nature: The F-distribution is used for right-tailed tests because it measures the ratio of variances, which are always non-negative.\n",
    "6. Dependence on Sample Size: Smaller sample sizes lead to a more skewed distribution, while larger sample sizes result in a more symmetric curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d972a26-3668-42b6-b389-f2af3d69ebe0",
   "metadata": {},
   "source": [
    "Qns 2) In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
    "Ans) Statistical Tests That Use the F-Distribution:\n",
    "The F-distribution is commonly used in statistical tests where the goal is to compare variances, test relationships, or assess model fits. It is appropriate for these tests because it evaluates ratios of variances, which are inherently non-negative and require skewed distributions. Below are the main types of tests:\n",
    "1. Analysis of Variance (ANOVA):\n",
    "To compare the means of three or more groups by analyzing the variability between group means relative to the variability within groups.\n",
    "ANOVA relies on the ratio of two variances: the between-group variance (variability due to differences among group means) and the within-group variance (variability within each group).\n",
    "The F-distribution is suitable because it evaluates this ratio.\n",
    "2. Regression Analysis:\n",
    "Purpose: To test the significance of the overall regression model or the contribution of individual predictors to the model.\n",
    "The F-statistic compares the variance explained by the regression model (model sum of squares) to the unexplained variance (residual sum of squares).\n",
    "It assesses whether the predictors significantly improve the model's fit to the data.\n",
    "3. F-Test for Equality of Variances:\n",
    "Purpose: To determine whether two populations have equal variances (homoscedasticity).\n",
    "The test is based on the ratio of the variances of two samples. The F-distribution naturally handles such variance ratios.\n",
    "4. MANOVA (Multivariate Analysis of Variance):\n",
    "Purpose: To test for differences in means across multiple dependent variables simultaneously.\n",
    "Like ANOVA, MANOVA uses variance ratios, but it extends the comparison to multiple dependent variables.\n",
    "5. General Linear Models:\n",
    "Purpose: To assess the fit of linear models that include multiple predictors and interactions.\n",
    "The F-statistic measures the ratio of the variance explained by the model to the unexplained variance, making it ideal for determining model significance.\n",
    "6. Nested Model Comparison:\n",
    "Purpose: To compare a more complex model (with additional parameters) to a simpler model to see if the added complexity improves the fit significantly.\n",
    "The F-test evaluates the change in variance explained by the models relative to their degrees of freedom.\n",
    "\n",
    "F-Distribution is Appropriate for These Tests:\n",
    "Handles Ratios: The F-distribution is designed to evaluate the ratio of two independent variances, which is a central feature of these tests.\n",
    "Non-Negative: Since variances are always non-negative, the F-distribution (which starts at 0) is appropriate.\n",
    "Depends on Degrees of Freedom: The shape of the F-distribution depends on the sample sizes (degrees of freedom), aligning with the needs of these tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745ec56a-280b-48ef-aab0-8deb6645f780",
   "metadata": {},
   "source": [
    "Qns 3) What are the key assumptions required for conducting an F-test to compare the variances of two \n",
    "populations?Ans) \n",
    "Key Assumptions for Conducting an F-Test to Compare Variances:\n",
    "An F-test is used to compare the variances of two populations to determine if they are significantly different. However, several assumptions must be met to ensure the validity of the test results. These assumptions are:\n",
    "\n",
    "1. Normality of the Populations:\n",
    "The populations from which the samples are drawn must follow a normal distribution.\n",
    "The F-test is sensitive to deviations from normality, and results can be misleading if this assumption is violated.\n",
    "2. Independence of Samples:\n",
    "The two samples being compared must be independent of each other.\n",
    "This means that the data from one sample should not influence or depend on the data from the other sample.\n",
    "3. Random Sampling:\n",
    "The samples must be randomly selected from the populations.\n",
    "This ensures that the results are representative of the populations being compared.\n",
    "4. Ratio of Variances is Non-Negative:\n",
    "Since variances are always non-negative, the ratio of the variances (used in the F-statistic) must also be non-negative.\n",
    "5. Equal Sample Sizes (Optional):\n",
    "While not strictly required, the F-test is more robust when the sample sizes of the two groups are approximately equal. Unequal sample sizes can make the test more sensitive to departures from normality.\n",
    "6. No Outliers:\n",
    "Outliers can significantly affect the variance, which may lead to incorrect results. It is important to check for and address outliers before performing the F-test\n",
    ".\n",
    "Consequences of Violating Assumptions:\n",
    "Violation of Normality: May lead to inaccurate p-values and incorrect conclusions. For non-normal data, alternative tests like the Levene's test or Bartlett's test are recommended.\n",
    "Violation of Independence: Results may not be valid, as the test assumes no relationship between the two samples.\n",
    "Violation of Random Sampling: The results may not generalize to the population\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b80d2a-b43e-4039-a1bf-288bf1592703",
   "metadata": {},
   "source": [
    "Qns 4) What is the purpose of ANOVA, and how does it differ from a t-test? \n",
    "Ans) Purpose of ANOVA (Analysis of Variance):\n",
    "ANOVA is used to compare the means of three or more groups to determine whether there is a statistically significant difference among them.\n",
    "\n",
    "Applications:\n",
    "It is commonly applied in experiments where multiple treatments or groups are compared.\n",
    "Examples include comparing the performance of different teaching methods, drug effectiveness, or machine productivity.\n",
    "ANOVA use: Performing multiple pairwise t-tests increases the risk of Type I error (false positives). ANOVA controls this error by testing all group means simultaneously.\n",
    "ANOVA provides an F-statistic and a p-value, indicating whether there is a significant difference among the group means.\n",
    "\n",
    "A t-test and ANOVA are both statistical methods used to compare group means, but they differ in scope and application. A t-test is designed for comparing the means of two groups, such as analyzing the difference in average height between males and females. It uses the t-statistic and assumes a null hypothesis that the two group means are equal. However, when multiple comparisons are performed using t-tests, the risk of Type I error (false positives) increases.\n",
    "\n",
    "On the other hand, ANOVA (Analysis of Variance) is used for comparing the means of three or more groups simultaneously. It tests the overall difference among group means using the F-statistic, which evaluates the ratio of variances. Unlike a t-test, ANOVA controls the Type I error rate when dealing with multiple groups. If ANOVA determines that at least one group mean is significantly different, post-hoc tests (e.g., Tukey's test) can be conducted to identify the specific groups that differ. For example, ANOVA is ideal for comparing the test scores of students across three different teaching methods. While t-tests focus on pairwise comparisons, ANOVA provides a single, robust test for multiple groups, making it a more efficient and reliable method in such scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c1932a-40fe-4f2c-a9c9-ec0e0c6e3720",
   "metadata": {},
   "source": [
    "Qns 5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.\n",
    "Ans) When to Use a One-Way ANOVA:\n",
    "More Than Two Groups:\n",
    "Use one-way ANOVA when you need to compare the means of three or more groups (e.g., comparing the effectiveness of three different teaching methods).\n",
    "\n",
    "Single Independent Variable:\n",
    "One-way ANOVA is appropriate when there is only one independent variable with multiple levels (e.g., teaching methods: A, B, and C).\n",
    "\n",
    "Why Use a One-Way ANOVA Instead of Multiple t-Tests?\n",
    "Controls Type I Error:\n",
    "\n",
    "Performing multiple t-tests increases the chance of Type I error (incorrectly rejecting the null hypothesis).\n",
    "For example, if you conduct three pairwise t-tests (e.g., Group A vs. B, Group B vs. C, Group A vs. C), the overall probability of making a Type I error is higher due to error accumulation.\n",
    "ANOVA performs a single test that controls the overall Type I error rate.\n",
    "Efficiency:\n",
    "\n",
    "One-way ANOVA is more efficient than performing multiple t-tests.\n",
    "Instead of conducting multiple calculations, ANOVA provides a single F-statistic to test the hypothesis.\n",
    "Interpretation:\n",
    "\n",
    "ANOVA tests whether there is a significant difference among all group means.\n",
    "Multiple t-tests only compare two groups at a time, making it harder to understand the overall relationship among all groups.\n",
    "Post-Hoc Analysis:\n",
    "\n",
    "If ANOVA detects a significant difference, post-hoc tests (e.g., Tukey’s test) can be used to identify specific group differences.\n",
    "This approach ensures a more structured and statistically valid comparison process.\n",
    "Example:\n",
    "Scenario: You want to compare the test scores of students taught using three methods:\n",
    "Group A: Traditional lectures.\n",
    "Group B: Online classes.\n",
    "Group C: Hybrid method.\n",
    "Using t-tests:\n",
    "You would need to compare:\n",
    "Group A vs. Group B.\n",
    "Group B vs. Group C.\n",
    "Group A vs. Group C.\n",
    "This results in three separate tests, each increasing the chance of a Type I error.\n",
    "Using ANOVA:\n",
    "One-way ANOVA evaluates whether there is any significant difference among the three groups in a single test, controlling the error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9eecd0-3ac5-4932-b2a8-6b0db505c6b4",
   "metadata": {},
   "source": [
    "Qns 6)  Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\r\n",
    "How does this partitioning contribute to the calculation of the F-statistic\n",
    "Ans) Variance Partitioning in ANOVA:\n",
    "In ANOVA, the total variance in the dataset is partitioned into two components: between-group variance and within-group variance. This partitioning helps determine whether the differences in group means are statistically significant.\n",
    "1. Between-Group Variance: The variation due to differences between the group means.\n",
    "It reflects the effect of the independent variable (e.g., treatment, group membership) on the dependent variable.\n",
    "It is based on how far the group means are from the overall mean of the dataset (grand mean).\n",
    "Larger between-group variance indicates greater differences among group means.\n",
    "\n",
    "3. Within-Group Variance:nThe variation within each group due to individual differences or random error.\n",
    "It reflects the variability of data points within each group around their respective group mean.\n",
    "It is based on the deviations of individual data points from their group mean.\n",
    "Smaller within-group variance indicates more consistency within groups.\n",
    "4. Total Variance: The overall variability in the data.\n",
    "Total variance is the sum of the between-group variance and within-group variance.\n",
    "\n",
    "If the null hypothesis (all group means are equal) is true, the between-group variance will be small relative to the within-group variance, resulting in a low F-value.\n",
    "If the null hypothesis is false (at least one group mean differs), the between-group variance will be large compared to the within-group variance, resulting in a high F-value.\n",
    "Interpretation:\n",
    "\n",
    "A higher F-value suggests that the group means are significantly different, leading to rejection of the null hypothesis.\n",
    "A low F-value suggests no significant difference among group means.?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22736b27-f301-4e6b-b604-f4d129c6fdb1",
   "metadata": {},
   "source": [
    "Qns 7) 7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\r\n",
    "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
    "\n",
    "Ans) Both the frequentist and Bayesian approaches can be used to analyze variance (ANOVA), but they differ significantly in terms of philosophy, handling of uncertainty, parameter estimation, and hypothesis testing.\n",
    "\n",
    "1. Philosophical Basis\n",
    "Classical (Frequentist) Approach:\n",
    "Relies on long-run frequencies of data.\n",
    "Assumes that parameters (e.g., group means, variances) are fixed but unknown.\n",
    "Uses data to test a null hypothesis without incorporating prior information.\n",
    "Bayesian Approach:\n",
    "Based on Bayes' theorem, which updates beliefs (prior distributions) about parameters using observed data.\n",
    "Assumes parameters are random variables with probability distributions that quantify uncertainty.\n",
    "2. Handling of Uncertainty\n",
    "Classical Approach\n",
    "\n",
    "Uncertainty is represented only by the sampling distribution of the test statistic (e.g., F-statistic).\n",
    "Provides a p-value to quantify how likely the observed data is, assuming the null hypothesis is true.\n",
    "No direct probability is assigned to hypotheses or parameter values.\n",
    "Bayesian Approach\n",
    "\n",
    "Uncertainty is expressed as probability distributions for parameters (posterior distributions).\n",
    "Produces credible intervals, which represent the range of parameter values with a specified probability (e.g., a 95% credible interval contains the true value 95% of the time, given the data and priors)\n",
    ".3.. Parameter Estimation\n",
    "Classical Approach\n",
    "\n",
    "Estimates parameters (e.g., group means) using point estimates (e.g., sample means).\n",
    "Confidence intervals are derived from the sampling distribution and are based on repeated sampling.\n",
    "Bayesian Approach\n",
    "\n",
    "Combines prior knowledge (prior distributions) with the data (likelihood) to produce posterior distributions for parameters.\n",
    "Allows for the incorporation of prior beliefs about parameters, which can influence results when data is limited\n",
    ".\n",
    "4. Hypothesis Testing\n",
    "Classical Approach\n",
    "\n",
    "Hypothesis testing involves comparing a null hypothesis (e.g., all group means are equal) with an alternative hypothesis.\n",
    "Provides a p-value to determine whether to reject the null hypothesis, based on a pre-specified significance level (e.g., 0.05).\n",
    "Bayesian Approach\n",
    "\n",
    "Hypothesis testing is done using posterior probabilities.\n",
    "Instead of rejecting or failing to reject a hypothesis, Bayesian ANOVA quantifies the probability of the null hypothesis and the alternative hypothesis directly.\n",
    "Can calculate Bayes factors, which measure the strength of evidence for one hypothesis over another\n",
    ".5. Interpretation\n",
    "Classical Approach:\n",
    "A significant result (e.g., p-value < 0.05) means the null hypothesis is unlikely, but it does not provide the probability that the null hypothesis is true.\n",
    "Results are binary (reject or fail to reject the null hypothesis).\n",
    "Bayesian Approach\n",
    "\n",
    "Results are more flexible and probabilistic.\n",
    "For example, you might state that \"there is a 90% probability that the difference between group means is greater than 5.\n",
    "\"76. Flexibility\n",
    "Classical Approach\n",
    "\n",
    "Less flexible in handling complex models, especially with small datasets or hierarchical structures.\n",
    "Bayesian Approach\n",
    "\n",
    "More flexible and better suited for complex or hierarchical models.\n",
    "Can incorporate prior information, which is particularly useful when data is sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574f1b16-dcbd-4542-89d8-2b322e4aaaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 2.089171974522293\n",
      "p-value: 0.4930485990053393\n",
      "Fail to reject the null hypothesis: Variances are equal.\n"
     ]
    }
   ],
   "source": [
    "#8. Question: You have two sets of data representing the incomes of two different professions1\n",
    "#V Profession A: [48, 52, 55, 60, 62'\n",
    "#V Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
    "#incomes are equal. What are your conclusions based on the F-test?\n",
    "#Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
    "#Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.\n",
    "    \n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "profession_A = [48, 52, 55, 60, 62]\n",
    "profession_B = [45, 50, 55, 52, 47]\n",
    "\n",
    "var_A = np.var(profession_A, ddof=1)  \n",
    "var_B = np.var(profession_B, ddof=1)  \n",
    "\n",
    "if var_A > var_B:\n",
    "    F_statistic = var_A / var_B\n",
    "    df1, df2 = len(profession_A) - 1, len(profession_B) - 1\n",
    "else:\n",
    "    F_statistic = var_B / var_A\n",
    "    df1, df2 = len(profession_B) - 1, len(profession_A) - 1\n",
    "\n",
    "p_value = stats.f.sf(F_statistic, df1, df2) * 2  #\n",
    "print(\"F-statistic:\", F_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "alpha = 0.05  \n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: Variances are not equal.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: Variances are equal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3ea890-f5b8-4fd9-82b3-1fc1b0ca5d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 67.87330316742101\n",
      "p-value: 2.870664187937026e-07\n",
      "Reject the null hypothesis: There is a significant difference in average heights between the regions.\n"
     ]
    }
   ],
   "source": [
    "#9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
    "#average heights between three different regions with the following data1\n",
    "#V Region A: [160, 162, 165, 158, 164'\n",
    "#V Region B: [172, 175, 170, 168, 174'\n",
    "#V Region C: [180, 182, 179, 185, 183'\n",
    "#V Task: Write Python code to perform the one-way ANOVA and interpret the results\f",
    "\n",
    "#V Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "region_A = [160, 162, 165, 158, 164]\n",
    "region_B = [172, 175, 170, 168, 174]\n",
    "region_C = [180, 182, 179, 185, 183]\n",
    "\n",
    "F_statistic, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
    "\n",
    "print(\"F-statistic:\", F_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "alpha = 0.05 \n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference in average heights between the regions.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference in average heights between the regions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28579928-c9d0-4927-8de3-200ff30c5b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
