{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "871f6601-8715-4078-b94a-26c4ffb8c7ff",
   "metadata": {},
   "source": [
    "Qns 1) What is a parameter?\n",
    "In programming and mathematics, a parameter is a variable used to pass information to a function, method, or procedure. Parameters are defined in the function signature and act as placeholders for the values (known as arguments) that are provided when the function is called."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc071b6-1fe5-45ce-8564-3210fbeef960",
   "metadata": {},
   "source": [
    "Qns 2)What is correlation? \n",
    "What does negative correlation mean\n",
    "Ans) Correlation is a statistical measure that describes the strength and direction of the relationship between two variables. It quantifies how changes in one variable are associated with changes in another.\n",
    "\n",
    "Negative correlation means that there is an inverse relationship between two variables. In other words:\n",
    "As one variable increases, the other decreases.\n",
    "Conversely, as one variable decreases, the other increases.?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec5caf-0f60-4482-91eb-4ffec7a683e4",
   "metadata": {},
   "source": [
    "Qns 3) Define Machine Learning. What are the main components in Machine Learning?\n",
    "Ans)Machine Learning (ML) is a branch of artificial intelligence (AI) that focuses on creating systems that can learn and make decisions or predictions based on data. Instead of being explicitly programmed with a set of rules, machine learning models identify patterns in data and improve their performance as they are exposed to more information.\n",
    "\n",
    "Main Components in Machine Learning\n",
    "The machine learning process involves several key components:\n",
    "\n",
    "Data:\n",
    "\n",
    "Data is the foundation of machine learning. It is used to train, validate, and test the model.\n",
    "Types of data: Structured (tables, rows, and columns) or unstructured (text, images, audio, etc.).\n",
    "Example: Sales data for predicting future trends.\n",
    "Features:\n",
    "\n",
    "Features are the measurable properties or characteristics of the data used as inputs for the model.\n",
    "Example: In a dataset of houses, features could be the size, number of rooms, and location.\n",
    "Feature engineering, which includes selecting, transforming, and creating features, is critical to improving model performance.\n",
    "Model:\n",
    "\n",
    "A machine learning model is an algorithm or mathematical function that maps inputs (features) to outputs (predictions).\n",
    "Models can vary based on the type of task (e.g., regression, classification, clustering).\n",
    "Training:\n",
    "\n",
    "Training is the process of teaching the model to recognize patterns in the data by minimizing the error in its predictions.\n",
    "During training, the model learns parameters (weights) using optimization techniques like gradient descent.\n",
    "Loss Function:\n",
    "\n",
    "The loss function quantifies the difference between the model's predictions and the actual target values.\n",
    "The goal of training is to minimize the loss function.\n",
    "Examples: Mean Squared Error (MSE) for regression, Cross-Entropy for classification.\n",
    "Optimization Algorithm:\n",
    "\n",
    "Optimization algorithms adjust the model's parameters to minimize the loss function.\n",
    "Example: Gradient Descent, Stochastic Gradient Descent (SGD), Adam.\n",
    "Validation:\n",
    "\n",
    "Validation involves testing the model on a separate dataset to ensure it generalizes well to unseen data.\n",
    "Techniques like cross-validation are used to evaluate the model's performance.\n",
    "Testing:\n",
    "\n",
    "Testing assesses the final model's accuracy and reliability using a third, unseen dataset (test set).\n",
    "Deployment:\n",
    "\n",
    "Once a model is trained and tested, it can be deployed in real-world applications for making predictions or decisions.\n",
    "Feedback Loop:\n",
    "\n",
    "Feedback from the deployed model’s performance is used to retrain or fine-tune the model to improve accuracy over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a587c8ac-4fd9-43c7-819b-767a2eea3ec3",
   "metadata": {},
   "source": [
    "Qns 4) How does loss value help in determining whether the model is good or not\n",
    "Ans) The loss value is a numerical representation of how well or poorly a machine learning model is performing on a given dataset. It measures the discrepancy between the model's predictions and the actual target values. A lower loss value indicates better performance, whereas a higher loss value suggests the model is not making accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc732df7-dfb5-4ebc-bbf1-a52afc80a614",
   "metadata": {},
   "source": [
    "Qns 5)What are continuous and categorical variables?\n",
    "Ans) Continuous Variables\n",
    "A continuous variable is a type of variable that can take on an infinite number of possible values within a range. These variables are typically numerical and are measured rather than counted.\n",
    "\n",
    "Categorical Variables\n",
    "A categorical variable is a type of variable that represents discrete categories or groups. These variables describe characteristics or qualities and do not have a meaningful numerical order (except for ordinal variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58628a36-107e-436f-865d-99c6b8f5afab",
   "metadata": {},
   "source": [
    "Qns 6) How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
    "Ans) Handling Categorical Variables in Machine Learning\n",
    "Categorical variables must be transformed into a format that machine learning models can understand (typically numerical). This process is known as encoding, and there are several techniques depending on the type of categorical data and the machine learning algorithm being used.\n",
    "\n",
    "Common Techniques for Handling Categorical Variables\n",
    "Label Encoding\n",
    "\n",
    "Assigns a unique integer to each category.\n",
    "One-Hot Encoding\n",
    "\n",
    "Converts categories into binary columns for each unique category.\n",
    "Binary Encoding\n",
    "\n",
    "Encodes categories into binary representations, reducing dimensionality compared to one-hot encoding.\n",
    "Ordinal Encoding\n",
    "\n",
    "Assigns integers to categories while preserving their order (suitable for ordinal variables).\n",
    "Frequency Encoding\n",
    "\n",
    "Replaces categories with their frequency in the dataset.\n",
    "Target Encoding (Mean Encoding)\n",
    "\n",
    "Encodes categories using the mean of the target variable for each category.\n",
    "Hash Encoding\n",
    "\n",
    "Maps categories into a fixed number of hash bins, useful for high-cardinality variables.\n",
    "Dummy Encoding\n",
    "\n",
    "A variation of one-hot encoding that drops one category to avoid multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99a27c-9070-4f08-a24f-f46c4adfb778",
   "metadata": {},
   "source": [
    "Qns 7) What do you mean by training and testing a dataset?\n",
    "Ans) Training: he training dataset is the portion of the data used to train the machine learning model. The model learns patterns, relationships, and features from this data to make predictions.\n",
    "Testing: The testing dataset is a separate portion of the data that is not used during training. It is used to evaluate the model's performance and measure its ability to generalize to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6041528c-9b08-4e57-aa3d-f4dbad1ea051",
   "metadata": {},
   "source": [
    "Qns 8) What is sklearn.preprocessing?\n",
    "Ans) sklearn.preprocessing is a module in the Scikit-learn library that provides a wide range of tools for data preprocessing and feature engineering in machine learning. These tools help transform raw data into a suitable format for machine learning algorithms to process effectively. Preprocessing ensures data is clean, normalized, scaled, and encoded to improve the performance and accuracy of machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2dd86a-7105-40cd-88b4-cb7e120505bc",
   "metadata": {},
   "source": [
    "Qns 9) What is a Test Set?\n",
    "A test set is a portion of the dataset used to evaluate the performance of a machine learning model after training.\n",
    "It contains data that the model has never seen, ensuring an unbiased assessment of its ability to generalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e514f5-e1e2-4e24-9122-f3ace09e9541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qns 10. How Do We Split Data for Model Fitting (Training and Testing) in Python?\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "How Do You Approach a Machine Learning Problem?\n",
    "ANs) Understand the Problem: Define objectives and constraints.\n",
    "Collect Data: Gather relevant datasets.\n",
    "Perform Exploratory Data Analysis (EDA): Explore relationships and clean the data.\n",
    "Preprocess Data: Handle missing values, encode categorical variables, scale numerical features.\n",
    "Feature Engineering: Create and select the most relevant features.\n",
    "Model Selection: Choose an appropriate algorithm.\n",
    "Training and Testing: Train the model on training data and evaluate it on the test set.\n",
    "Tuning: Optimize hyperparameters using techniques like Grid Search or Random Search.\n",
    "Deployment: Integrate the model into a production environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a800821-b159-4870-a882-a4a66f8ed929",
   "metadata": {},
   "source": [
    "Qns 11) Why Do We Perform EDA Before Fitting a Model?\n",
    "Ans) To understand the data distribution, detect outliers, and identify missing values.\n",
    "To visualize relationships and patterns among features.\n",
    "To select the most relevant features for modeling.\n",
    "To avoid introducing biases or errors during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4f9d3c-1ddc-4040-a60f-b37a6946a53c",
   "metadata": {},
   "source": [
    "Qns 12) What is Correlation?\n",
    "Ans) Correlation quantifies the degree to which two variables move together.\n",
    "Values range from -1 (perfect negative correlation) to 1 (perfect positive correlation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec28c30f-37c3-4fb4-9c62-a1b2c8381845",
   "metadata": {},
   "source": [
    "Qns 13) What Does Negative Correlation Mean?\n",
    "Ans) As one variable increases, the other decreases (e.g., as temperature rises, the sale of winter jackets decreases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13cd255c-4203-411f-a644-e7eead8841bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Variable1  Variable2  Variable3\n",
      "Variable1        1.0        1.0       -1.0\n",
      "Variable2        1.0        1.0       -1.0\n",
      "Variable3       -1.0       -1.0        1.0\n"
     ]
    }
   ],
   "source": [
    "#Qns 14) How can you find correlation between variables in Python?\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    \"Variable1\": [1, 2, 3, 4, 5],\n",
    "    \"Variable2\": [2, 4, 6, 8, 10],\n",
    "    \"Variable3\": [5, 4, 3, 2, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c3462-cb5a-4e42-91bd-61914df61ec3",
   "metadata": {},
   "source": [
    "Qns 15) What is causation? Explain difference between correlation and causation with an example\n",
    "Ans) Causation refers to a relationship between two variables where a change in one variable directly causes a change in the other. It implies a cause-and-effect relationship.\n",
    "\n",
    "Example: Correlation Without Causation\n",
    "There is a strong positive correlation between the number of ice creams sold and drowning incidents during summer.\n",
    "Reason for Correlation: Both events are linked to a common factor: hot weather. People eat more ice cream and go swimming in summer, which increases the likelihood of drowning incidents.\n",
    "No Causation: Eating ice cream does not cause drowning.\n",
    "\n",
    "Example: Causation\n",
    "Smoking and lung cancer.\n",
    "Causation: Research shows that smoking causes lung cancer by introducing harmful chemicals into the lungs that damage cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694ac54c-37a6-4d36-8c26-58a6275b635e",
   "metadata": {},
   "source": [
    "Qns 16) What is an Optimizer? What are different types of optimizers? Explain each with an example\n",
    "Ans)An optimizer adjusts model parameters (weights and biases) to minimize the loss function during training, ensuring efficient learning.\n",
    "\n",
    "Types of Optimizers\n",
    "1. Gradient Descent\n",
    "Minimizes the loss by updating parameters in the direction of the negative gradient.\n",
    "Variants: Batch, Stochastic (SGD), Mini-batch.\n",
    "Example: weights = weights - learning_rate * gradient.\n",
    "\n",
    "2. Momentum\n",
    "Accelerates convergence and reduces oscillations by adding momentum to updates.\n",
    "Example:\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "3. Adagrad\n",
    "Adjusts learning rates based on historical gradients.\n",
    "Example:\n",
    "izer = tf.keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "\n",
    "4. RMSprop\n",
    "Maintains an exponentially decaying average of squared gradients for adaptive learning rates.\n",
    "Example:\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "5. Adam\n",
    "Combines Momentum and RMSprop, using adaptive learning rates and momentum.\n",
    "Example:\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "6. Nadam\n",
    "Extends Adam with Nesterov momentum for better convergence.\n",
    "Example:\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "\n",
    "7. AdaDelta\n",
    "Limits the accumulation of past squared gradients for better updates.\n",
    "Example:\n",
    "optimizer = tf.keras.optimizers.Adadelta(learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe632d66-a4ae-4f2c-984d-63c31bf3161b",
   "metadata": {},
   "source": [
    "Qns 17) What is sklearn.linear_model ?\n",
    "Ans) The sklearn.linear_model module in scikit-learn contains classes and functions for implementing linear models for regression and classification tasks. These models are based on the assumption that the relationship between the input variables (features) and the output variable is linear.\n",
    "Key Features\n",
    "Provides tools for both regression and classification.\n",
    "Supports regularization techniques like Lasso, Ridge, and ElasticNet.\n",
    "Optimized for efficiency and ease of use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5356d0-6390-4710-bdac-40d6ee080912",
   "metadata": {},
   "source": [
    "Qns 18) What does model.fit() do? What arguments must be given?\n",
    "Ans) The fit() method in scikit-learn is used to train a machine learning model. It fits the model to the provided training data by learning the relationships between the input features (X) and the target variable (y). This process involves calculating the parameters (weights and biases) of the model that minimize the loss function for the training data.\n",
    "                                                                                                                                     Arguments for model.fit()\n",
    "Input Features (X)\n",
    "\n",
    "Type: Array-like (e.g., NumPy array, Pandas DataFrame).\n",
    "Shape: (n_samples, n_features)\n",
    "Represents input data for prediction.\n",
    "Target Variable (y)\n",
    "\n",
    "Type: Array-like (e.g., NumPy array, Pandas Series).\n",
    "Shape: (n_samples,)\n",
    "Contains ground-truth values (continuous for regression, class labels for classification).\n",
    "Sample Weights (sample_weight) (optional)\n",
    "\n",
    "Type: Array-like, shape (n_samples,)\n",
    "Assigns importance to individual samples during training.                                                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c4d00a-ac3b-4754-84f2-0621a228f201",
   "metadata": {},
   "source": [
    "Qns 19) What does model.predict() do? What arguments must be given?\n",
    "Ans) model.predict() generates predictions for input data (X) based on the trained model. It outputs the predicted target values (e.g., labels or continuous values).\n",
    "\n",
    "Arguments for model.predict()\n",
    "Input Features (X)\n",
    "Type: Array-like (e.g., NumPy array, Pandas DataFrame).\n",
    "Shape: (n_samples, n_features)\n",
    "Represents new data for which predictions are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e324cd3a-3545-4b14-ba6f-7a7da5c9ce0a",
   "metadata": {},
   "source": [
    "Qns 20) What are continuous and categorical variables?\n",
    "Ans) Continuous Variables\n",
    "Definition: Variables that can take any value within a range. They are measured on a scale and have infinite possible values.\n",
    "Examples: Height, weight, temperature, age, income.\n",
    "Key Property: Typically represented by numerical data and used in regression tasks.\n",
    "    \n",
    "Categorical Variables\n",
    "Definition: Variables that represent distinct groups or categories. They have a limited number of values and do not have an inherent order (unless ordinal).\n",
    "Examples: Gender (male, female), color (red, green, blue), product type.\n",
    "Key Property: Represented as labels or categories and used in classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e655ca40-b316-41b6-bd17-2be9e786dbc6",
   "metadata": {},
   "source": [
    "Qns 21) What is feature scaling? How does it help in Machine Learning?\n",
    "Ans) Feature scaling is the process of normalizing or standardizing the range of independent variables (features) in a dataset. It ensures that all features contribute equally to the model by bringing them to a similar scale.\n",
    "\n",
    "How Feature Scaling Helps in Machine Learning\n",
    "Speeds Up Convergence: Helps gradient descent algorithms converge faster by ensuring uniform feature impact.\n",
    "Improves Accuracy: Prevents bias caused by features with larger magnitudes.\n",
    "Better Model Interpretability: Makes the model coefficients more meaningful, especially in linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd48aa-e991-4c7b-bd31-b514bbd4fd5e",
   "metadata": {},
   "source": [
    "Qns 22) How do we perform scaling in Python?\n",
    "Ans) \n",
    "1. Standardization (Z-score normalization)\n",
    "This scales the data to have a mean of 0 and a standard deviation of 1.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # X is your input data\n",
    "fit_transform(): Computes the mean and standard deviation, then scales the data.\n",
    "\n",
    "2. Normalization (Min-Max scaling)\n",
    "This scales the data to a specific range, typically [0, 1].\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # X is your input data\n",
    "fit_transform(): Computes the minimum and maximum values and scales the data accordingly.\n",
    "\n",
    "3. Robust Scaling (for outlier-resistant scaling)\n",
    "This method scales the data based on median and interquartile range (IQR), making it robust to outliers.\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # X is your input data\n",
    "\n",
    "4. Scaling After Splitting Data\n",
    "If you’re working with a training and testing split, always fit the scaler on the training data and then transform both the training and testing data.\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932da77a-00b1-47aa-bf43-e19447eaff47",
   "metadata": {},
   "source": [
    "Qns 23) What is sklearn.preprocessing?\n",
    "Ans) sklearn.preprocessing is a module in scikit-learn that provides functions and classes to preprocess data, such as scaling, encoding, and transforming input features to make them suitable for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bdfba3-890a-418f-aeb8-0a3f20fb9c52",
   "metadata": {},
   "source": [
    "Qns 24) How do we split data for model fitting (training and testing) in Python?\n",
    "Ans) In Python, we can split data into training and testing sets using the train_test_split function from sklearn.model_selection. This ensures that we train the model on one subset of data and evaluate its performance on another subse\n",
    "\n",
    "Steps to Split Data:\n",
    "Import train_test_split:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Call train_test_split:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82791aa7-2f51-402e-8ed3-0b21e1ad25f3",
   "metadata": {},
   "source": [
    "Qns 25)Explain data encoding?\n",
    "ANs) Data encoding is the process of converting categorical (non-numeric) data into a numerical format that machine learning models can understand. Since most machine learning algorithms work with numerical data, encoding categorical variables is essential for preprocessing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
